# Zero-Shot Modeller
Zero-Shot Modeller (ZSM), a is two-phased approach that efficiently examines the mutant space using machine learning (ML) guidance without requiring high-throughput methods. ZSM combines the specific information of small datasets with the general knowledge of pretrained zero-shot predictors to discover enhanced protein variants, thereby removing the need large datasets common for traditional ML techniques.

ZSM provides the nescessary tools to easily implement all elements of the workflow. In the **Exploration Phase**, the protein information of an experimental low-N dataset is parsed through a diverse collection of zero-shot models based on the [ProteinGym](https://github.com/OATML-Markslab/ProteinGym) code. The resulting predicted scores are correlated with the original experimental values, identifying the model most suitable for predicting better mutants. In the **Exploitation Phase**, ZSM can be used to create, predict, and rank a large _in silico_ mutant dataset using the models identified as most suitable in the previous phase. Importantly, visualization tools are also provided, such as the plotting of individual correlations, comparisons of model performances, or the construction of single-mutant landscapes. For more information on all tools available in ZSM, please see the example notebook.

Importantly, the best models identified by ZSM consistently outperforms the worst models across three different enzyme benchmark datasets. Furthermore, while ZSM was developed for enzyme engineering, the approach also exhibits improved results for a diverse selection of non-enzymatic protein properties.

## Collection of Zero-shot models.
ZSM leverage pre-trained zero-shot models published by other groups and we reserve no rights to the work nor code:
| Model           | Model Input  | Repository URL                                                                                      | Reference                                                                                              |
|------------------|--------------|-----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| ESM-1b          | Sequence     | [https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm)                 | [Rives, A. et al. (2021). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS, 118.](https://www.pnas.org/doi/10.1073/pnas.2016239118)                                     |
| ESM-1v          | Sequence     | [https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm)                 | [Meier, J. et al. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.](https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html) |
| ESM-2           | Sequence     | [https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm)                 | [Lin, Z et al. (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379.](https://www.science.org/doi/10.1126/science.ade2574) |
| ProGen2         | Sequence     | [https://github.com/salesforce/progen](https://github.com/salesforce/progen)                       | [Nijkamp, E. et al. (2023). ProGen2: Exploring the Boundaries of Protein Language Models. Cell Systems.](https://www.sciencedirect.com/science/article/pii/S2405471223002727) |
| ProtGPT2        | Sequence     | [https://huggingface.co/nferruz/ProtGPT2](https://huggingface.co/nferruz/ProtGPT2)                 | [Ferruz, N. et al. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.](https://www.nature.com/articles/s41467-022-32007-7) |
| RITA            | Sequence     | [https://github.com/lightonai/RITA](https://github.com/lightonai/RITA)                             | [Hesslow, D. et al. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv.](https://arxiv.org/abs/2205.05789) |
| Tranception     | Sequence     | [https://github.com/OATML-Markslab/Tranception](https://github.com/OATML-Markslab/Tranception)     | [Notin, P. el al. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.](https://proceedings.mlr.press/v162/notin22a.html) |
| UniRep          | Sequence     | [https://github.com/churchlab/UniRep](https://github.com/churchlab/UniRep)                         | [Alley, E.C. et al. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods.](https://www.nature.com/articles/s41592-019-0598-1)     |
| EVE             | MSA          | [https://github.com/OATML-Markslab/EVE](https://github.com/OATML-Markslab/EVE)                     | [Frazer, J. et al. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.](https://www.nature.com/articles/s41586-021-04043-8) |
| eUniRep        | MSA          | [https://github.com/chloechsu/combining-evolutionary-and-assay-labelled-data](https://github.com/chloechsu/combining-evolutionary-and-assay-labelled-data) | [Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods.](https://www.nature.com/articles/s41592-019-0598-1)     |
| TranceptEVE     | MSA          | [https://github.com/OATML-Markslab/ProteinGym](https://github.com/OATML-Markslab/ProteinGym)       | [Notin, P. et al (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.](https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1) |                                                              |
| ESM-IF1         | Structure    | [https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm)                 | [Hsu, C et al. (2022). Learning Inverse Folding from Millions of Predicted Structures. ICML.](https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2.full) |
| ProteinMPNN     | Structure    | [https://github.com/dauparas/ProteinMPNN](https://github.com/dauparas/ProteinMPNN)                 | [Dauparas, J. et al. (2022). Robust deep learning-based protein sequence design using ProteinMPNN. Science, 378.](https://www.science.org/doi/10.1126/science.add2187) |


